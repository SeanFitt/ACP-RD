{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--model_name_or_path",
                "roberta-large",
                "--per_device_train_batch_size",
                "32",
                "--evaluation_strategy",
                "epoch",
                "--overwrite_output_dir",
                "--num_train_epochs",
                "30",
                "--do_train",
                "--max_seq_length",
                "128",
                "--save_strategy",
                "no",
                "--hidden_dropout_prob",
                "0.1",
                "--learning_rate",
                "7e-3",
                "--seed",
                "11",
                "--prefix",
                "--task_name",
                "glue",
                "--output_dir",
                "checkpoints/rte-roberta/",
                "--dataset_name",
                "rte",
                "--do_eval",
                "--pre_seq_len",
                "8"
            ]
        },
        {
            "name": "AL on PHEME",
            "type": "python",
            "request": "launch",
            "program": "run.py",
            "console": "integratedTerminal",
            "args": [
                "--model_name_or_path",
                "pre_trained_model/bert-base-uncased",
                "--overwrite_output_dir",
                "--per_device_train_batch_size",
                "32",
                "--do_active_learning",
                "--evaluation_strategy",
                "epoch",
                "--num_train_epochs",
                "3",
                "--do_train",
                "--max_seq_length",
                "128",
                "--save_strategy",
                "no",             
                "--hidden_dropout_prob",
                "0.1",
                "--learning_rate",
                "5e-5",
                "--seed",
                "32",
                "--task_name",
                "rumor_detection",
                "--output_dir",
                "checkpoints/unsp_bert_based_uncased_pheme/",
                "--dataset_name",
                "pheme",
                "--pre_seq_len",
                "8",
                "--init_set",
                "200",
                "--metric",
                "f1",
                "--device",
                "mps"

            ]
        },
        {
            "name": "P-tuning on PHEME",
            "type": "python",
            "request": "launch",
            "program": "run.py",
            "console": "integratedTerminal",
            "args": [
                "--model_name_or_path",
                "bert-base-uncased",
                "--prefix",
                "--overwrite_output_dir",
                "--per_device_train_batch_size",
                "32",
                "--evaluation_strategy",
                "epoch",
                "--num_train_epochs",
                "20",
                "--do_active_learning",
                "--do_train",
                "--max_seq_length",
                "128",
                "--save_strategy",
                "no",             
                "--hidden_dropout_prob",
                "0.1",
                "--learning_rate",
                "7e-3",
                "--seed",
                "32",
                "--task_name",
                "rumor_detection",
                "--output_dir",
                "checkpoints/bert-base-uncased/",
                "--dataset_name",
                "pheme",
                "--pre_seq_len",
                "8",
                "--active_budget",
                "50",
                "--per_active_budget",
                "50"
            ]
        }
    ]
}